\subsection{Extensions to LDS models}
\label{sec:bestlds:background:extensions}

In addition to standard approaches, there has also been substantial work extending LDS models to more flexible dynamical systems in order to better capture important features in neuroscience data. For example, Gaussian-process factor analysis (GPFA) was developed to provide insight into neural signals by extracting smooth, low-dimensional trajectories from recorded activity \cite{yu_gaussian-process_2009}. Building on GPFA, Orthogonal stochastic linear mixing models (OSLMMs) relax the assumption that correlations across neurons are time-invariant and provide innovations in the regression framework that makes inference more tractable for large datasets \cite{meng_bayesian_2022}. Other approaches employ LDS models as constituents of multi-component frameworks in order to capture more complex temporal activity patterns in neural data. One class of examples includes latent factor analysis via dynamical systems (LFADS) and its extensions, which use recurrent neural networks (RNNs) to recover neural population dynamics \cite{pandarinath_inferring_2018, prince_parallel_2021, zhu_deep_2022}. Another approach employs variants of switching linear dynamical systems (sLDS), which learn flexible, non-linear models of neural and behavioral data by treating activity as sequences of repeated dynamical modes \cite{linderman_hierarchical_2019, nassar_tree-structured_2019, zoltowski_general_2020, mudrik_decomposed_2022, wang_bayesian_2022, weinreb_keypoint-moseq_2023}. These advances make clear that LDS models serve a valuable and fundamental purpose in neuroscience and will continue to act as an essential building block for future work. 