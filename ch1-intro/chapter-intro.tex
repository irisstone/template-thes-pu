\chapter{Introduction\label{ch:intro}}

Characterizing the dynamic structure underlying the complex behavior of mammals is a central goal of neuroscience \cite{gomez-marin_big_2014, krakauer_neuroscience_2017, niv_primacy_2021}. As recording technology improves and scientists grow capable of collecting ever-increasing amounts of data, the sophistication of the techniques used to analyze such data must grow along with it. To meet this need, a new field of research, often broadly referred to as `computational ethology' or `behavioral quantification' has emerged that calls for the use of statistical, mathematical, and computational tools to identify hidden structure in animal behavior \cite{anderson_toward_2014, egnor_computational_2016, brown_ethology_2018, berman_measuring_2018, mathis_deep_2020}. By developing robust, reliable measures of quantifying behavior, it will be possible to then map those behaviors onto underlying patterns of neural activity, thus revealing new insights into how the brain elicits all manner of cognitive processes \cite{datta_computational_2019, pereira_quantifying_2020}. One tool that has become particularly popular for characterizing animal behavior -- and linking it to the brain -- is the latent variable model. Latent variable models are a class of unsupervised and semi-supervised learning methods (i.e. methods that require no or minimal user-directed input) that offer interpretable explanations about the relationships between observed variables, such as direct measurements of behavior, in terms of hidden variables that are not directly observed \cite{muthen_beyond_2002, skrondal_latent_2007, everett_introduction_2013}. 

In the behavioral context, these hidden variables may represent any number of features of an animal's activity. For example, they may simply represent the discretized structure of an animal's actions on a particular timescale, such as in one seminal study identifying the distinct `syllables' that comprise exploratory behavior as mice move about an open arena \cite{wiltschko_mapping_2015}. Creative applications of such models to invertebrates have uncovered yet more complex latent variables representing motivational states and neural activity that, while not directly observed, manifest in the animal's behavior. This includes past work that has identified different sensorimotor strategies used during fly courtship \cite{calhoun_unsupervised_2019} and the continuous dynamics of neural activity in \textit{C. elegans} during navigational behavior \cite{linderman_hierarchical_2019}. While these examples deal primarily with freely-moving, relatively unconstrained behavior, there is also plenty of evidence to suggest that such models can uncover insightful structure in task-oriented decision-making. Monkeys and rodents in particular are known to exhibit `lapses' in attention \cite{pisupati_lapses_2021}, change their strategies over time \cite{roy_extracting_2021}, and succumb to internal drivers such as satiety and fatigue \cite{carandini_probing_2013} as they perform tasks -- all aspects of behavior which may readily be described by latent factors. 

As the field of computational ethology explodes, it is becoming clear where the biggest opportunities are in improving our descriptions of the dynamic structure of behavior such as to be more accurate and comprehensive. For one, no animal acts in a vacuum, and while there have been a few studies that have incorporated external sensory information into models of behavior in limited settings, such as fly courtship \cite{calhoun_unsupervised_2019} and rat decision-making \cite{roy_extracting_2021}, there has been a need for substantial further investigation into how input-driven latent variable models can answer basic questions about how animals are performing tasks and responding to their environment in a structured way. There is yet further need to link these models to some understanding of underlying neural function, whether that be by incorporating neural activity into the models directly or by combining the behavior with neural perturbation studies.  

Recent work in this area has also revealed a number of pain-points that currently preclude widespread adoption of these methods for quantifying behavior. In the case of naturalistic behavior, where the observations typically take the form of postural information about the animal, such as `joint positions' or `key points' extracted from animal tracking software \cite{mathis_deeplabcut_2018, pereira_fast_2019}, noisiness in the extracted positional markers can quickly degrade the quality of any subsequent analysis that uses those markers as observations. There is thus a need for approaches that circumvent this issue in a time-expedient fashion. Devising solutions that are compute- and time-efficient is particularly important given how costly traditional inference techniques \cite{baum_maximization_1970, dempster_maximum_1977, shumway_approach_1982, escola_hidden_2011, metropolis_equation_1953, hastings_monte_1970, geman_stochastic_1984, gelfand_sampling-based_1990} for learning the parameters of latent variable systems can be on both measures. This is indicative of yet another opportunity: the development of efficient fitting methods that can accurately learn or approximate the system parameters of different latent variable models so as to make methods of quantifying behavior of all types -- from decision-making to naturalistic exploration -- more efficient. 

In this thesis, we develop a number of latent variable models for characterizing the dynamic structure underlying complex mammalian behavior, with a focus on models that address each of the gaps in the field mentioned above. That is, each of the chapters covers, in some combination or another, models of behavior that are input-driven, enable the investigation of neural correlates, address problems of noise in the measured observations, and/or provide tools for more efficient fitting of the associated system parameters. 

In Chapter 2, we introduce a hybrid model that combines a Bernoulli Generalized Linear Model (GLM) with a Hidden Markov Model (HMM) in order to describe mouse decision-making behavior as they navigate a virtual maze during a cognitively demanding task. The HMM component of the model provides a way to cluster task trials into discrete states, while incorporating GLM weights enabled us to ascertain the role of different inputs (e.g. sensory stimuli and previous choice behavior) on the animals' decision-making in each of those states. Importantly, on a subset of trials the mice were inhibited unilaterally in either the direct or indirect pathway of their dorsomedial striatum (DMS), and we incorporated the presence of inhibition into the model as a GLM input. This approach led to one of the crucial findings of the project: three states, each corresponding to a different decision-making strategy, that had differing dependence on the DMS. We believe this result opens up a new line of inquiry into the possibility that the neural correlates of decision-making -- and perhaps other cognitive processes as well -- are not static, but rather dynamic in time according to an animal's changing internal state. This work was a joint effort with my co-first author Scott Bolkan, several other co-authors, and my advisors Ilana Witten and Jonathan Pillow. It was first presented at the Computational and Systems Neuroscience (COSYNE) Conference in 2020 under the title ``Latent-state models reveal a state-dependent contribution of the striatum to decision-making" and ultimately published in the journal \textit{Nature Neuroscience} in 2022 as ``Opponent control of behavior by dorsomedial striatal pathways depends on task demands and internal state" \cite{bolkan_opponent_2022}. It is reproduced here with permission from Springer Nature. 

In Chapter 3, we extend the concept of input-driven, discrete state models from binary decision-making tasks to the high-dimensional, continuous-time setting of natural behavior. We compare two latent variable models for quantifying unconstrained behavior. The first is an autoregressive hidden Markov model (AR-HMM), which can be seen as a version of a GLM-HMM (generically called an input-output hidden Markov model or IO-HMM) wherein the inputs are the observations at previous time points and the mapping between inputs and observations is linear. The second is a switching linear dynamical system (SLDS), which is an extention of the AR-HMM with an additional layer of continuous latents between the discrete states and observations. In this work, we show that the SLDS provides superior performance over the AR-HMM on a number of measures and discuss ways to further improve upon the SLDS framework, many of which are in the process of being implemented in other studies \cite{weinreb_keypoint-moseq_2023}. We also propose a new initialization technique for fitting SLDS models and lay the groundwork for subsequent advances in this area. While there are many remaining avenues of study related to this work, we leave them as open questions for future researchers. This work was a joint effort with Victoria Corbit, who collected the data used in the modeling, as well as my advisors Ilana Witten and Jonathan Pillow. 

In Chapter 4, we more thoroughly explore the concept of initialization techniques to improve computational efficiency in latent variable models, with a particular focus on Bernoulli linear dynamical systems. Bernoulli LDS models have much in common with the Bernoulli GLM-HMM discussed in Chapter \ref{ch:glmhmm}, with the primary distinction being that the latent state trajectories are continuous rather than discrete. Such models are useful for identifying the temporal dynamics underlying time series data such as decision-making, as in the GLM-HMM, as well as discrete stochastic processes
such as binned neural spike trains. Here we develop a spectral learning method for fast,
efficient fitting of Bernoulli latent LDS models that we call bestLDS (BErnoulli SpecTral Linear Dynamical System). Our approach extends traditional subspace identification methods to the Bernoulli setting via a transformation of the first and second sample moments. We find that bestLDS provides parameter estimates that can entirely replace iterative fitting procedures like the expectation-maximization (EM) algorithm or else provides good initialization for EM fitting. We also demonstrate that bestLDS provides substantial benefits to real world settings by analyzing the same mouse sensory decision-making data as in Chapter \ref{ch:glmhmm}. This work was a joint effort with my co-first author Yotam Sagiv, our collaborator Memming Park, and my advisor Jonathan Pillow. It was first presented at the Computational and Systems Neuroscience (COSYNE) Conference in 2023 under the title ``Spectral learning of Bernoulli linear dynamical systems models for decision-making" and ultimately published in the journal \textit{Transactions on Machine Learning Research} in 2023 as ``Spectral learning of Bernoulli linear dynamical systems models" [\textbf{INSERT CITATION}]. It is reproduced here with permission from JMLR, Inc. 